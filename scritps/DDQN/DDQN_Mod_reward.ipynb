{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "EtcRrbm0dwME"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAnkpBIIdnt_",
        "outputId": "4f1fdf34-22f3-4ee8-a0f4-02981c5de3cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Suggested packages:\n",
            "  swig3.0-examples swig3.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig3.0\n",
            "0 upgraded, 1 newly installed, 0 to remove and 1 not upgraded.\n",
            "Need to get 1,109 kB of archives.\n",
            "After this operation, 5,555 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig3.0 amd64 3.0.12-2.2ubuntu1 [1,109 kB]\n",
            "Fetched 1,109 kB in 1s (942 kB/s)\n",
            "Selecting previously unselected package swig3.0.\n",
            "(Reading database ... 117926 files and directories currently installed.)\n",
            "Preparing to unpack .../swig3.0_3.0.12-2.2ubuntu1_amd64.deb ...\n",
            "Unpacking swig3.0 (3.0.12-2.2ubuntu1) ...\n",
            "Setting up swig3.0 (3.0.12-2.2ubuntu1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Requirement already satisfied: gym[box2d] in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (3.0.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (0.0.8)\n",
            "Collecting box2d-py==2.3.5 (from gym[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pygame==2.1.0 (from gym[box2d])\n",
            "  Downloading pygame-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting swig==4.* (from gym[box2d])\n",
            "  Downloading swig-4.2.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m70.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2309695 sha256=d182117415d261b36957018af3f9f51452a05b4225483910846b2c01e75a34b7\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: swig, box2d-py, pygame\n",
            "  Attempting uninstall: pygame\n",
            "    Found existing installation: pygame 2.5.2\n",
            "    Uninstalling pygame-2.5.2:\n",
            "      Successfully uninstalled pygame-2.5.2\n",
            "Successfully installed box2d-py-2.3.5 pygame-2.1.0 swig-4.2.1\n"
          ]
        }
      ],
      "source": [
        "!apt-get install swig3.0\n",
        "!ln -s /usr/bin/swig3.0 /usr/bin/swig\n",
        "!pip install gym[box2d]\n",
        "!pip install pyvirtualdisplay --quiet\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!apt-get install -y swig build-essential python-dev python3-dev > /dev/null 2>&1\n",
        "!apt-get install x11-utils > /dev/null 2>&1\n",
        "!apt-get install xvfb > /dev/null 2>&1"
      ],
      "metadata": {
        "id": "xzXETOHlDXPy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install swig3.0\n",
        "!ln -s /usr/bin/swig3.0 /usr/bin/swig\n",
        "!pip install gym[box2d]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8lfWfK8Db2j",
        "outputId": "0aec7c87-8f91-4e33-c1a5-444f81514a62"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "swig3.0 is already the newest version (3.0.12-2.2ubuntu1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 1 not upgraded.\n",
            "ln: failed to create symbolic link '/usr/bin/swig': File exists\n",
            "Requirement already satisfied: gym[box2d] in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (3.0.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (0.0.8)\n",
            "Requirement already satisfied: box2d-py==2.3.5 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (2.3.5)\n",
            "Requirement already satisfied: pygame==2.1.0 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (2.1.0)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (4.2.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rarfile --quiet\n",
        "!pip install stable-baselines3[extra] --quiet\n",
        "!pip install ale-py --quiet\n",
        "!pip install gym[box2d] --quiet\n",
        "!pip install pyvirtualdisplay --quiet\n",
        "!pip install pyglet --quiet\n",
        "!pip install pygame --quiet\n",
        "!pip install minigrid --quiet\n",
        "!pip install -q swig --quiet\n",
        "!pip install -q gymnasium[box2d] --quiet\n",
        "!pip install 'minigrid<=2.1.1' --quiet\n",
        "!pip3 install box2d-py --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkAtaCaUDgtu",
        "outputId": "2fd43202-6288-4729-b853-823db5bfc269"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.3/182.3 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m884.3/884.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.8/103.8 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.3/100.3 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Imports\n",
        "import io\n",
        "import os\n",
        "import glob\n",
        "import torch\n",
        "import base64\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import sys\n",
        "import gymnasium\n",
        "sys.modules[\"gym\"] = gymnasium\n",
        "\n",
        "# import stable_baselines3\n",
        "# from stable_baselines3 import DQN\n",
        "# # from stable_baselines3 import\n",
        "# from stable_baselines3.common.results_plotter import ts2xy, load_results\n",
        "# from stable_baselines3.common.callbacks import EvalCallback\n",
        "# from stable_baselines3.common.env_util import make_atari_env\n",
        "\n",
        "import gymnasium as gym\n",
        "from gym import spaces\n",
        "from gym.envs.box2d.lunar_lander import *\n",
        "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
        "from google.colab import files\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Sequential\n",
        "from collections import deque\n",
        "import numpy as np\n",
        "from gym.wrappers.monitoring.video_recorder import VideoRecorder"
      ],
      "metadata": {
        "id": "pZW05P_vd57n"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir results/\n",
        "!mkdir saved/\n",
        "!mkdir videos/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-aMo-QRxH87U",
        "outputId": "2b5dae83-a675-4c08-f3ec-85479e344ea1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Play Video function\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "from pyvirtualdisplay import Display\n",
        "import os\n",
        "\n",
        "# create the directory to store the video(s)\n",
        "os.makedirs(\"./video\", exist_ok=True)\n",
        "\n",
        "display = Display(visible=False, size=(1400, 900))\n",
        "_ = display.start()\n",
        "\n",
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment\n",
        "and displaying it.\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "def render_mp4(videopath: str) -> str:\n",
        "  \"\"\"\n",
        "  Gets a string containing a b4-encoded version of the MP4 video\n",
        "  at the specified path.\n",
        "  \"\"\"\n",
        "  mp4 = open(videopath, 'rb').read()\n",
        "  base64_encoded_mp4 = b64encode(mp4).decode()\n",
        "  return f'<video width=400 controls><source src=\"data:video/mp4;' \\\n",
        "         f'base64,{base64_encoded_mp4}\" type=\"video/mp4\"></video>'\n",
        "\n",
        "\n",
        "\n",
        "# #ٌRandom behaviour here\n",
        "# env_name = 'LunarLander-v2'\n",
        "# env = gym.make(env_name)\n",
        "\n",
        "# env = gym.make(env_name, render_mode=\"rgb_array\")\n",
        "# vid = VideoRecorder(env, path=f\"video/{env_name}_first_run.mp4\")\n",
        "# observation = env.reset()[0]\n",
        "\n",
        "# total_reward = 0\n",
        "# done = False\n",
        "# while not done:\n",
        "#   frame = env.render()\n",
        "#   # print(frame)\n",
        "#   vid.capture_frame()\n",
        "#   action = np.random.randint(4)\n",
        "#   observation, reward, done, info, _ = env.step(action)\n",
        "#   total_reward += reward\n",
        "# vid.close()\n",
        "# env.close()\n",
        "# print(f\"\\nTotal reward: {total_reward}\")\n",
        "\n",
        "# # show video\n",
        "# html = render_mp4(f\"video/{env_name}_first_run.mp4\")\n",
        "# HTML(html)"
      ],
      "metadata": {
        "id": "MFU3--CYQKLv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DQN on basic environment"
      ],
      "metadata": {
        "id": "_s0mPibPf8mF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wReYF9tQgGM2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3a9c2f7-9d65-46e0-c37c-00084c7f0e0b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vtAT34v0E9Al"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DDQN Modified reward\n"
      ],
      "metadata": {
        "id": "gVoOuxQdE_RM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import math\n",
        "import warnings\n",
        "from typing import TYPE_CHECKING, Optional\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import gym\n",
        "from gym import error, spaces\n",
        "from gym.error import DependencyNotInstalled\n",
        "from gym.utils import EzPickle, colorize\n",
        "from gym.utils.step_api_compatibility import step_api_compatibility\n",
        "\n",
        "try:\n",
        "    import Box2D\n",
        "    from Box2D.b2 import (\n",
        "        circleShape,\n",
        "        contactListener,\n",
        "        edgeShape,\n",
        "        fixtureDef,\n",
        "        polygonShape,\n",
        "        revoluteJointDef,\n",
        "    )\n",
        "except ImportError:\n",
        "    raise DependencyNotInstalled(\"box2d is not installed, run `pip install gym[box2d]`\")\n",
        "\n",
        "\n",
        "if TYPE_CHECKING:\n",
        "    import pygame\n",
        "\n",
        "\n",
        "FPS = 50\n",
        "SCALE = 30.0  # affects how fast-paced the game is, forces should be adjusted as well\n",
        "\n",
        "MAIN_ENGINE_POWER = 13.0\n",
        "SIDE_ENGINE_POWER = 0.6\n",
        "\n",
        "INITIAL_RANDOM = 1000.0  # Set 1500 to make game harder\n",
        "\n",
        "LANDER_POLY = [(-14, +17), (-17, 0), (-17, -10), (+17, -10), (+17, 0), (+14, +17)]\n",
        "LEG_AWAY = 20\n",
        "LEG_DOWN = 18\n",
        "LEG_W, LEG_H = 2, 8\n",
        "LEG_SPRING_TORQUE = 40\n",
        "\n",
        "SIDE_ENGINE_HEIGHT = 14.0\n",
        "SIDE_ENGINE_AWAY = 12.0\n",
        "\n",
        "VIEWPORT_W = 600\n",
        "VIEWPORT_H = 400\n",
        "\n",
        "\n",
        "class ContactDetector(contactListener):\n",
        "    def __init__(self, env):\n",
        "        contactListener.__init__(self)\n",
        "        self.env = env\n",
        "\n",
        "    def BeginContact(self, contact):\n",
        "        if (\n",
        "            self.env.lander == contact.fixtureA.body\n",
        "            or self.env.lander == contact.fixtureB.body\n",
        "        ):\n",
        "            self.env.game_over = True\n",
        "        for i in range(2):\n",
        "            if self.env.legs[i] in [contact.fixtureA.body, contact.fixtureB.body]:\n",
        "                self.env.legs[i].ground_contact = True\n",
        "\n",
        "    def EndContact(self, contact):\n",
        "        for i in range(2):\n",
        "            if self.env.legs[i] in [contact.fixtureA.body, contact.fixtureB.body]:\n",
        "                self.env.legs[i].ground_contact = False\n",
        "\n",
        "\n",
        "class CustomLunarLander(gym.Env, EzPickle):\n",
        "    \"\"\"\n",
        "    ### Description\n",
        "    This environment is a classic rocket trajectory optimization problem.\n",
        "    According to Pontryagin's maximum principle, it is optimal to fire the\n",
        "    engine at full throttle or turn it off. This is the reason why this\n",
        "    environment has discrete actions: engine on or off.\n",
        "\n",
        "    There are two environment versions: discrete or continuous.\n",
        "    The landing pad is always at coordinates (0,0). The coordinates are the\n",
        "    first two numbers in the state vector.\n",
        "    Landing outside of the landing pad is possible. Fuel is infinite, so an agent\n",
        "    can learn to fly and then land on its first attempt.\n",
        "\n",
        "    To see a heuristic landing, run:\n",
        "    ```\n",
        "    python gym/envs/box2d/lunar_lander.py\n",
        "    ```\n",
        "    <!-- To play yourself, run: -->\n",
        "    <!-- python examples/agents/keyboard_agent.py LunarLander-v2 -->\n",
        "\n",
        "    ### Action Space\n",
        "    There are four discrete actions available: do nothing, fire left\n",
        "    orientation engine, fire main engine, fire right orientation engine.\n",
        "\n",
        "    ### Observation Space\n",
        "    The state is an 8-dimensional vector: the coordinates of the lander in `x` & `y`, its linear\n",
        "    velocities in `x` & `y`, its angle, its angular velocity, and two booleans\n",
        "    that represent whether each leg is in contact with the ground or not.\n",
        "\n",
        "    ### Rewards\n",
        "    After every step a reward is granted. The total reward of an episode is the\n",
        "    sum of the rewards for all the steps within that episode.\n",
        "\n",
        "    For each step, the reward:\n",
        "    - is increased/decreased the closer/further the lander is to the landing pad.\n",
        "    - is increased/decreased the slower/faster the lander is moving.\n",
        "    - is decreased the more the lander is tilted (angle not horizontal).\n",
        "    - is increased by 10 points for each leg that is in contact with the ground.\n",
        "    - is decreased by 0.03 points each frame a side engine is firing.\n",
        "    - is decreased by 0.3 points each frame the main engine is firing.\n",
        "\n",
        "    The episode receive an additional reward of -100 or +100 points for crashing or landing safely respectively.\n",
        "\n",
        "    An episode is considered a solution if it scores at least 200 points.\n",
        "\n",
        "    ### Starting State\n",
        "    The lander starts at the top center of the viewport with a random initial\n",
        "    force applied to its center of mass.\n",
        "\n",
        "    ### Episode Termination\n",
        "    The episode finishes if:\n",
        "    1) the lander crashes (the lander body gets in contact with the moon);\n",
        "    2) the lander gets outside of the viewport (`x` coordinate is greater than 1);\n",
        "    3) the lander is not awake. From the [Box2D docs](https://box2d.org/documentation/md__d_1__git_hub_box2d_docs_dynamics.html#autotoc_md61),\n",
        "        a body which is not awake is a body which doesn't move and doesn't\n",
        "        collide with any other body:\n",
        "    > When Box2D determines that a body (or group of bodies) has come to rest,\n",
        "    > the body enters a sleep state which has very little CPU overhead. If a\n",
        "    > body is awake and collides with a sleeping body, then the sleeping body\n",
        "    > wakes up. Bodies will also wake up if a joint or contact attached to\n",
        "    > them is destroyed.\n",
        "\n",
        "    ### Arguments\n",
        "    To use to the _continuous_ environment, you need to specify the\n",
        "    `continuous=True` argument like below:\n",
        "    ```python\n",
        "    import gym\n",
        "    env = gym.make(\n",
        "        \"LunarLander-v2\",\n",
        "        continuous: bool = False,\n",
        "        gravity: float = -10.0,\n",
        "        enable_wind: bool = False,\n",
        "        wind_power: float = 15.0,\n",
        "        turbulence_power: float = 1.5,\n",
        "    )\n",
        "    ```\n",
        "    If `continuous=True` is passed, continuous actions (corresponding to the throttle of the engines) will be used and the\n",
        "    action space will be `Box(-1, +1, (2,), dtype=np.float32)`.\n",
        "    The first coordinate of an action determines the throttle of the main engine, while the second\n",
        "    coordinate specifies the throttle of the lateral boosters.\n",
        "    Given an action `np.array([main, lateral])`, the main engine will be turned off completely if\n",
        "    `main < 0` and the throttle scales affinely from 50% to 100% for `0 <= main <= 1` (in particular, the\n",
        "    main engine doesn't work  with less than 50% power).\n",
        "    Similarly, if `-0.5 < lateral < 0.5`, the lateral boosters will not fire at all. If `lateral < -0.5`, the left\n",
        "    booster will fire, and if `lateral > 0.5`, the right booster will fire. Again, the throttle scales affinely\n",
        "    from 50% to 100% between -1 and -0.5 (and 0.5 and 1, respectively).\n",
        "\n",
        "    `gravity` dictates the gravitational constant, this is bounded to be within 0 and -12.\n",
        "\n",
        "    If `enable_wind=True` is passed, there will be wind effects applied to the lander.\n",
        "    The wind is generated using the function `tanh(sin(2 k (t+C)) + sin(pi k (t+C)))`.\n",
        "    `k` is set to 0.01.\n",
        "    `C` is sampled randomly between -9999 and 9999.\n",
        "\n",
        "    `wind_power` dictates the maximum magnitude of linear wind applied to the craft. The recommended value for `wind_power` is between 0.0 and 20.0.\n",
        "    `turbulence_power` dictates the maximum magnitude of rotational wind applied to the craft. The recommended value for `turbulence_power` is between 0.0 and 2.0.\n",
        "\n",
        "    ### Version History\n",
        "    - v2: Count energy spent and in v0.24, added turbulance with wind power and turbulence_power parameters\n",
        "    - v1: Legs contact with ground added in state vector; contact with ground\n",
        "        give +10 reward points, and -10 if then lose contact; reward\n",
        "        renormalized to 200; harder initial random push.\n",
        "    - v0: Initial version\n",
        "\n",
        "    <!-- ### References -->\n",
        "\n",
        "    ### Credits\n",
        "    Created by Oleg Klimov\n",
        "    \"\"\"\n",
        "\n",
        "    metadata = {\n",
        "        \"render_modes\": [\"human\", \"rgb_array\"],\n",
        "        \"render_fps\": FPS,\n",
        "    }\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        render_mode: Optional[str] = None,\n",
        "        continuous: bool = False,\n",
        "        gravity: float = -10.0,\n",
        "        enable_wind: bool = False,\n",
        "        wind_power: float = 15.0,\n",
        "        turbulence_power: float = 1.5,\n",
        "    ):\n",
        "        EzPickle.__init__(\n",
        "            self,\n",
        "            render_mode,\n",
        "            continuous,\n",
        "            gravity,\n",
        "            enable_wind,\n",
        "            wind_power,\n",
        "            turbulence_power,\n",
        "        )\n",
        "\n",
        "        assert (\n",
        "            -12.0 < gravity and gravity < 0.0\n",
        "        ), f\"gravity (current value: {gravity}) must be between -12 and 0\"\n",
        "        self.gravity = gravity\n",
        "\n",
        "        if 0.0 > wind_power or wind_power > 20.0:\n",
        "            warnings.warn(\n",
        "                colorize(\n",
        "                    f\"WARN: wind_power value is recommended to be between 0.0 and 20.0, (current value: {wind_power})\",\n",
        "                    \"yellow\",\n",
        "                ),\n",
        "            )\n",
        "        self.wind_power = wind_power\n",
        "\n",
        "        if 0.0 > turbulence_power or turbulence_power > 2.0:\n",
        "            warnings.warn(\n",
        "                colorize(\n",
        "                    f\"WARN: turbulence_power value is recommended to be between 0.0 and 2.0, (current value: {turbulence_power})\",\n",
        "                    \"yellow\",\n",
        "                ),\n",
        "            )\n",
        "        self.turbulence_power = turbulence_power\n",
        "\n",
        "        self.enable_wind = enable_wind\n",
        "        self.wind_idx = np.random.randint(-9999, 9999)\n",
        "        self.torque_idx = np.random.randint(-9999, 9999)\n",
        "\n",
        "        self.screen: pygame.Surface = None\n",
        "        self.clock = None\n",
        "        self.isopen = True\n",
        "        self.world = Box2D.b2World(gravity=(0, gravity))\n",
        "        self.moon = None\n",
        "        self.lander: Optional[Box2D.b2Body] = None\n",
        "        self.particles = []\n",
        "\n",
        "        self.prev_reward = None\n",
        "\n",
        "        self.continuous = continuous\n",
        "\n",
        "        low = np.array(\n",
        "            [\n",
        "                # these are bounds for position\n",
        "                # realistically the environment should have ended\n",
        "                # long before we reach more than 50% outside\n",
        "                -1.5,\n",
        "                -1.5,\n",
        "                # velocity bounds is 5x rated speed\n",
        "                -5.0,\n",
        "                -5.0,\n",
        "                -math.pi,\n",
        "                -5.0,\n",
        "                -0.0,\n",
        "                -0.0,\n",
        "            ]\n",
        "        ).astype(np.float32)\n",
        "        high = np.array(\n",
        "            [\n",
        "                # these are bounds for position\n",
        "                # realistically the environment should have ended\n",
        "                # long before we reach more than 50% outside\n",
        "                1.5,\n",
        "                1.5,\n",
        "                # velocity bounds is 5x rated speed\n",
        "                5.0,\n",
        "                5.0,\n",
        "                math.pi,\n",
        "                5.0,\n",
        "                1.0,\n",
        "                1.0,\n",
        "            ]\n",
        "        ).astype(np.float32)\n",
        "\n",
        "        # useful range is -1 .. +1, but spikes can be higher\n",
        "        self.observation_space = spaces.Box(low, high)\n",
        "\n",
        "        if self.continuous:\n",
        "            # Action is two floats [main engine, left-right engines].\n",
        "            # Main engine: -1..0 off, 0..+1 throttle from 50% to 100% power. Engine can't work with less than 50% power.\n",
        "            # Left-right:  -1.0..-0.5 fire left engine, +0.5..+1.0 fire right engine, -0.5..0.5 off\n",
        "            self.action_space = spaces.Box(-1, +1, (2,), dtype=np.float32)\n",
        "        else:\n",
        "            # Nop, fire left engine, main engine, right engine\n",
        "            self.action_space = spaces.Discrete(4)\n",
        "\n",
        "        self.render_mode = render_mode\n",
        "\n",
        "    def _destroy(self):\n",
        "        if not self.moon:\n",
        "            return\n",
        "        self.world.contactListener = None\n",
        "        self._clean_particles(True)\n",
        "        self.world.DestroyBody(self.moon)\n",
        "        self.moon = None\n",
        "        self.world.DestroyBody(self.lander)\n",
        "        self.lander = None\n",
        "        self.world.DestroyBody(self.legs[0])\n",
        "        self.world.DestroyBody(self.legs[1])\n",
        "\n",
        "    def reset(\n",
        "        self,\n",
        "        *,\n",
        "        seed: Optional[int] = None,\n",
        "        options: Optional[dict] = None,\n",
        "    ):\n",
        "        super().reset(seed=seed)\n",
        "        self._destroy()\n",
        "        self.world.contactListener_keepref = ContactDetector(self)\n",
        "        self.world.contactListener = self.world.contactListener_keepref\n",
        "        self.game_over = False\n",
        "        self.prev_shaping = None\n",
        "\n",
        "        W = VIEWPORT_W / SCALE\n",
        "        H = VIEWPORT_H / SCALE\n",
        "\n",
        "        # terrain\n",
        "        CHUNKS = 11\n",
        "        height = self.np_random.uniform(0, H / 2, size=(CHUNKS + 1,))\n",
        "        chunk_x = [W / (CHUNKS - 1) * i for i in range(CHUNKS)]\n",
        "        self.helipad_x1 = chunk_x[CHUNKS // 2 - 1]\n",
        "        self.helipad_x2 = chunk_x[CHUNKS // 2 + 1]\n",
        "        self.helipad_y = H / 4\n",
        "        height[CHUNKS // 2 - 2] = self.helipad_y\n",
        "        height[CHUNKS // 2 - 1] = self.helipad_y\n",
        "        height[CHUNKS // 2 + 0] = self.helipad_y\n",
        "        height[CHUNKS // 2 + 1] = self.helipad_y\n",
        "        height[CHUNKS // 2 + 2] = self.helipad_y\n",
        "        smooth_y = [\n",
        "            0.33 * (height[i - 1] + height[i + 0] + height[i + 1])\n",
        "            for i in range(CHUNKS)\n",
        "        ]\n",
        "\n",
        "        self.moon = self.world.CreateStaticBody(\n",
        "            shapes=edgeShape(vertices=[(0, 0), (W, 0)])\n",
        "        )\n",
        "        self.sky_polys = []\n",
        "        for i in range(CHUNKS - 1):\n",
        "            p1 = (chunk_x[i], smooth_y[i])\n",
        "            p2 = (chunk_x[i + 1], smooth_y[i + 1])\n",
        "            self.moon.CreateEdgeFixture(vertices=[p1, p2], density=0, friction=0.1)\n",
        "            self.sky_polys.append([p1, p2, (p2[0], H), (p1[0], H)])\n",
        "\n",
        "        self.moon.color1 = (0.0, 0.0, 0.0)\n",
        "        self.moon.color2 = (0.0, 0.0, 0.0)\n",
        "\n",
        "        initial_y = VIEWPORT_H / SCALE\n",
        "        self.lander: Box2D.b2Body = self.world.CreateDynamicBody(\n",
        "            position=(VIEWPORT_W / SCALE / 2, initial_y),\n",
        "            angle=0.0,\n",
        "            fixtures=fixtureDef(\n",
        "                shape=polygonShape(\n",
        "                    vertices=[(x / SCALE, y / SCALE) for x, y in LANDER_POLY]\n",
        "                ),\n",
        "                density=5.0,\n",
        "                friction=0.1,\n",
        "                categoryBits=0x0010,\n",
        "                maskBits=0x001,  # collide only with ground\n",
        "                restitution=0.0,\n",
        "            ),  # 0.99 bouncy\n",
        "        )\n",
        "        self.lander.color1 = (128, 102, 230)\n",
        "        self.lander.color2 = (77, 77, 128)\n",
        "        self.lander.ApplyForceToCenter(\n",
        "            (\n",
        "                self.np_random.uniform(-INITIAL_RANDOM, INITIAL_RANDOM),\n",
        "                self.np_random.uniform(-INITIAL_RANDOM, INITIAL_RANDOM),\n",
        "            ),\n",
        "            True,\n",
        "        )\n",
        "\n",
        "        self.legs = []\n",
        "        for i in [-1, +1]:\n",
        "            leg = self.world.CreateDynamicBody(\n",
        "                position=(VIEWPORT_W / SCALE / 2 - i * LEG_AWAY / SCALE, initial_y),\n",
        "                angle=(i * 0.05),\n",
        "                fixtures=fixtureDef(\n",
        "                    shape=polygonShape(box=(LEG_W / SCALE, LEG_H / SCALE)),\n",
        "                    density=1.0,\n",
        "                    restitution=0.0,\n",
        "                    categoryBits=0x0020,\n",
        "                    maskBits=0x001,\n",
        "                ),\n",
        "            )\n",
        "            leg.ground_contact = False\n",
        "            leg.color1 = (128, 102, 230)\n",
        "            leg.color2 = (77, 77, 128)\n",
        "            rjd = revoluteJointDef(\n",
        "                bodyA=self.lander,\n",
        "                bodyB=leg,\n",
        "                localAnchorA=(0, 0),\n",
        "                localAnchorB=(i * LEG_AWAY / SCALE, LEG_DOWN / SCALE),\n",
        "                enableMotor=True,\n",
        "                enableLimit=True,\n",
        "                maxMotorTorque=LEG_SPRING_TORQUE,\n",
        "                motorSpeed=+0.3 * i,  # low enough not to jump back into the sky\n",
        "            )\n",
        "            if i == -1:\n",
        "                rjd.lowerAngle = (\n",
        "                    +0.9 - 0.5\n",
        "                )  # The most esoteric numbers here, angled legs have freedom to travel within\n",
        "                rjd.upperAngle = +0.9\n",
        "            else:\n",
        "                rjd.lowerAngle = -0.9\n",
        "                rjd.upperAngle = -0.9 + 0.5\n",
        "            leg.joint = self.world.CreateJoint(rjd)\n",
        "            self.legs.append(leg)\n",
        "\n",
        "        self.drawlist = [self.lander] + self.legs\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            self.render()\n",
        "        return self.step(np.array([0, 0]) if self.continuous else 0)[0], {}\n",
        "\n",
        "    def _create_particle(self, mass, x, y, ttl):\n",
        "        p = self.world.CreateDynamicBody(\n",
        "            position=(x, y),\n",
        "            angle=0.0,\n",
        "            fixtures=fixtureDef(\n",
        "                shape=circleShape(radius=2 / SCALE, pos=(0, 0)),\n",
        "                density=mass,\n",
        "                friction=0.1,\n",
        "                categoryBits=0x0100,\n",
        "                maskBits=0x001,  # collide only with ground\n",
        "                restitution=0.3,\n",
        "            ),\n",
        "        )\n",
        "        p.ttl = ttl\n",
        "        self.particles.append(p)\n",
        "        self._clean_particles(False)\n",
        "        return p\n",
        "\n",
        "    def _clean_particles(self, all):\n",
        "        while self.particles and (all or self.particles[0].ttl < 0):\n",
        "            self.world.DestroyBody(self.particles.pop(0))\n",
        "\n",
        "    def step(self, action):\n",
        "        assert self.lander is not None\n",
        "\n",
        "        # Update wind\n",
        "        assert self.lander is not None, \"You forgot to call reset()\"\n",
        "        if self.enable_wind and not (\n",
        "            self.legs[0].ground_contact or self.legs[1].ground_contact\n",
        "        ):\n",
        "            # the function used for wind is tanh(sin(2 k x) + sin(pi k x)),\n",
        "            # which is proven to never be periodic, k = 0.01\n",
        "            wind_mag = (\n",
        "                math.tanh(\n",
        "                    math.sin(0.02 * self.wind_idx)\n",
        "                    + (math.sin(math.pi * 0.01 * self.wind_idx))\n",
        "                )\n",
        "                * self.wind_power\n",
        "            )\n",
        "            self.wind_idx += 1\n",
        "            self.lander.ApplyForceToCenter(\n",
        "                (wind_mag, 0.0),\n",
        "                True,\n",
        "            )\n",
        "\n",
        "            # the function used for torque is tanh(sin(2 k x) + sin(pi k x)),\n",
        "            # which is proven to never be periodic, k = 0.01\n",
        "            torque_mag = math.tanh(\n",
        "                math.sin(0.02 * self.torque_idx)\n",
        "                + (math.sin(math.pi * 0.01 * self.torque_idx))\n",
        "            ) * (self.turbulence_power)\n",
        "            self.torque_idx += 1\n",
        "            self.lander.ApplyTorque(\n",
        "                (torque_mag),\n",
        "                True,\n",
        "            )\n",
        "\n",
        "        if self.continuous:\n",
        "            action = np.clip(action, -1, +1).astype(np.float32)\n",
        "        else:\n",
        "            assert self.action_space.contains(\n",
        "                action\n",
        "            ), f\"{action!r} ({type(action)}) invalid \"\n",
        "\n",
        "        # Engines\n",
        "        tip = (math.sin(self.lander.angle), math.cos(self.lander.angle))\n",
        "        side = (-tip[1], tip[0])\n",
        "        dispersion = [self.np_random.uniform(-1.0, +1.0) / SCALE for _ in range(2)]\n",
        "\n",
        "        m_power = 0.0\n",
        "        if (self.continuous and action[0] > 0.0) or (\n",
        "            not self.continuous and action == 2\n",
        "        ):\n",
        "            # Main engine\n",
        "            if self.continuous:\n",
        "                m_power = (np.clip(action[0], 0.0, 1.0) + 1.0) * 0.5  # 0.5..1.0\n",
        "                assert m_power >= 0.5 and m_power <= 1.0\n",
        "            else:\n",
        "                m_power = 1.0\n",
        "            # 4 is move a bit downwards, +-2 for randomness\n",
        "            ox = tip[0] * (4 / SCALE + 2 * dispersion[0]) + side[0] * dispersion[1]\n",
        "            oy = -tip[1] * (4 / SCALE + 2 * dispersion[0]) - side[1] * dispersion[1]\n",
        "            impulse_pos = (self.lander.position[0] + ox, self.lander.position[1] + oy)\n",
        "            p = self._create_particle(\n",
        "                3.5,  # 3.5 is here to make particle speed adequate\n",
        "                impulse_pos[0],\n",
        "                impulse_pos[1],\n",
        "                m_power,\n",
        "            )  # particles are just a decoration\n",
        "            p.ApplyLinearImpulse(\n",
        "                (ox * MAIN_ENGINE_POWER * m_power, oy * MAIN_ENGINE_POWER * m_power),\n",
        "                impulse_pos,\n",
        "                True,\n",
        "            )\n",
        "            self.lander.ApplyLinearImpulse(\n",
        "                (-ox * MAIN_ENGINE_POWER * m_power, -oy * MAIN_ENGINE_POWER * m_power),\n",
        "                impulse_pos,\n",
        "                True,\n",
        "            )\n",
        "\n",
        "        s_power = 0.0\n",
        "        if (self.continuous and np.abs(action[1]) > 0.5) or (\n",
        "            not self.continuous and action in [1, 3]\n",
        "        ):\n",
        "            # Orientation engines\n",
        "            if self.continuous:\n",
        "                direction = np.sign(action[1])\n",
        "                s_power = np.clip(np.abs(action[1]), 0.5, 1.0)\n",
        "                assert s_power >= 0.5 and s_power <= 1.0\n",
        "            else:\n",
        "                direction = action - 2\n",
        "                s_power = 1.0\n",
        "            ox = tip[0] * dispersion[0] + side[0] * (\n",
        "                3 * dispersion[1] + direction * SIDE_ENGINE_AWAY / SCALE\n",
        "            )\n",
        "            oy = -tip[1] * dispersion[0] - side[1] * (\n",
        "                3 * dispersion[1] + direction * SIDE_ENGINE_AWAY / SCALE\n",
        "            )\n",
        "            impulse_pos = (\n",
        "                self.lander.position[0] + ox - tip[0] * 17 / SCALE,\n",
        "                self.lander.position[1] + oy + tip[1] * SIDE_ENGINE_HEIGHT / SCALE,\n",
        "            )\n",
        "            p = self._create_particle(0.7, impulse_pos[0], impulse_pos[1], s_power)\n",
        "            p.ApplyLinearImpulse(\n",
        "                (ox * SIDE_ENGINE_POWER * s_power, oy * SIDE_ENGINE_POWER * s_power),\n",
        "                impulse_pos,\n",
        "                True,\n",
        "            )\n",
        "            self.lander.ApplyLinearImpulse(\n",
        "                (-ox * SIDE_ENGINE_POWER * s_power, -oy * SIDE_ENGINE_POWER * s_power),\n",
        "                impulse_pos,\n",
        "                True,\n",
        "            )\n",
        "\n",
        "        self.world.Step(1.0 / FPS, 6 * 30, 2 * 30)\n",
        "\n",
        "        pos = self.lander.position\n",
        "        vel = self.lander.linearVelocity\n",
        "        state = [\n",
        "            (pos.x - VIEWPORT_W / SCALE / 2) / (VIEWPORT_W / SCALE / 2),\n",
        "            (pos.y - (self.helipad_y + LEG_DOWN / SCALE)) / (VIEWPORT_H / SCALE / 2),\n",
        "            vel.x * (VIEWPORT_W / SCALE / 2) / FPS,\n",
        "            vel.y * (VIEWPORT_H / SCALE / 2) / FPS,\n",
        "            self.lander.angle,\n",
        "            20.0 * self.lander.angularVelocity / FPS,\n",
        "            1.0 if self.legs[0].ground_contact else 0.0,\n",
        "            1.0 if self.legs[1].ground_contact else 0.0,\n",
        "        ]\n",
        "        assert len(state) == 8\n",
        "\n",
        "        # Compare with / without shaping, referring the state description below\n",
        "        '''\n",
        "        state[0]: the horizontal coordinate\n",
        "        state[1]: the vertical coordinate\n",
        "        state[2]: the horizontal speed\n",
        "        state[3]: the vertical speed\n",
        "        state[4]: the angle\n",
        "        state[5]: the angular speed\n",
        "        state[6]: first leg contact\n",
        "        state[7]: second leg contact\n",
        "        '''\n",
        "        reward = 0\n",
        "        shaping = (\n",
        "            -150 * np.sqrt(state[0] * state[0] + state[1] * state[1])\n",
        "            - 50 * np.sqrt(state[2] * state[2] + state[3] * state[3])\n",
        "            - 100 * abs(state[4])\n",
        "            # -10* abs(state[5])\n",
        "            + 20 * state[6]\n",
        "            + 20 * state[7]\n",
        "        )  # And ten points for legs contact, the idea is if you\n",
        "        # lose contact again after landing, you get negative reward\n",
        "        if self.prev_shaping is not None:\n",
        "            reward = shaping - self.prev_shaping\n",
        "        self.prev_shaping = shaping\n",
        "\n",
        "        reward -= (\n",
        "            m_power * 0.30\n",
        "        )  # less fuel spent is better, about -30 for heuristic landing\n",
        "        reward -= s_power * 0.03\n",
        "\n",
        "        terminated = False\n",
        "        if self.game_over or abs(state[0]) >= 1.0:\n",
        "            terminated = True\n",
        "            reward = -100\n",
        "        if not self.lander.awake:\n",
        "            terminated = True\n",
        "            reward = +200\n",
        "        if reward <-400:\n",
        "          self.game_over=True\n",
        "          # terminated=True\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            self.render()\n",
        "        return np.array(state, dtype=np.float32), reward, terminated, False, {}\n",
        "\n",
        "    def render(self):\n",
        "        if self.render_mode is None:\n",
        "            gym.logger.warn(\n",
        "                \"You are calling render method without specifying any render mode. \"\n",
        "                \"You can specify the render_mode at initialization, \"\n",
        "                f'e.g. gym(\"{self.spec.id}\", render_mode=\"rgb_array\")'\n",
        "            )\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            import pygame\n",
        "            from pygame import gfxdraw\n",
        "        except ImportError:\n",
        "            raise DependencyNotInstalled(\n",
        "                \"pygame is not installed, run `pip install gym[box2d]`\"\n",
        "            )\n",
        "\n",
        "        if self.screen is None and self.render_mode == \"human\":\n",
        "            pygame.init()\n",
        "            pygame.display.init()\n",
        "            self.screen = pygame.display.set_mode((VIEWPORT_W, VIEWPORT_H))\n",
        "        if self.clock is None:\n",
        "            self.clock = pygame.time.Clock()\n",
        "\n",
        "        self.surf = pygame.Surface((VIEWPORT_W, VIEWPORT_H))\n",
        "\n",
        "        pygame.transform.scale(self.surf, (SCALE, SCALE))\n",
        "        pygame.draw.rect(self.surf, (255, 255, 255), self.surf.get_rect())\n",
        "\n",
        "        for obj in self.particles:\n",
        "            obj.ttl -= 0.15\n",
        "            obj.color1 = (\n",
        "                int(max(0.2, 0.15 + obj.ttl) * 255),\n",
        "                int(max(0.2, 0.5 * obj.ttl) * 255),\n",
        "                int(max(0.2, 0.5 * obj.ttl) * 255),\n",
        "            )\n",
        "            obj.color2 = (\n",
        "                int(max(0.2, 0.15 + obj.ttl) * 255),\n",
        "                int(max(0.2, 0.5 * obj.ttl) * 255),\n",
        "                int(max(0.2, 0.5 * obj.ttl) * 255),\n",
        "            )\n",
        "\n",
        "        self._clean_particles(False)\n",
        "\n",
        "        for p in self.sky_polys:\n",
        "            scaled_poly = []\n",
        "            for coord in p:\n",
        "                scaled_poly.append((coord[0] * SCALE, coord[1] * SCALE))\n",
        "            pygame.draw.polygon(self.surf, (0, 0, 0), scaled_poly)\n",
        "            gfxdraw.aapolygon(self.surf, scaled_poly, (0, 0, 0))\n",
        "\n",
        "        for obj in self.particles + self.drawlist:\n",
        "            for f in obj.fixtures:\n",
        "                trans = f.body.transform\n",
        "                if type(f.shape) is circleShape:\n",
        "                    pygame.draw.circle(\n",
        "                        self.surf,\n",
        "                        color=obj.color1,\n",
        "                        center=trans * f.shape.pos * SCALE,\n",
        "                        radius=f.shape.radius * SCALE,\n",
        "                    )\n",
        "                    pygame.draw.circle(\n",
        "                        self.surf,\n",
        "                        color=obj.color2,\n",
        "                        center=trans * f.shape.pos * SCALE,\n",
        "                        radius=f.shape.radius * SCALE,\n",
        "                    )\n",
        "\n",
        "                else:\n",
        "                    path = [trans * v * SCALE for v in f.shape.vertices]\n",
        "                    pygame.draw.polygon(self.surf, color=obj.color1, points=path)\n",
        "                    gfxdraw.aapolygon(self.surf, path, obj.color1)\n",
        "                    pygame.draw.aalines(\n",
        "                        self.surf, color=obj.color2, points=path, closed=True\n",
        "                    )\n",
        "\n",
        "                for x in [self.helipad_x1, self.helipad_x2]:\n",
        "                    x = x * SCALE\n",
        "                    flagy1 = self.helipad_y * SCALE\n",
        "                    flagy2 = flagy1 + 50\n",
        "                    pygame.draw.line(\n",
        "                        self.surf,\n",
        "                        color=(255, 255, 255),\n",
        "                        start_pos=(x, flagy1),\n",
        "                        end_pos=(x, flagy2),\n",
        "                        width=1,\n",
        "                    )\n",
        "                    pygame.draw.polygon(\n",
        "                        self.surf,\n",
        "                        color=(204, 204, 0),\n",
        "                        points=[\n",
        "                            (x, flagy2),\n",
        "                            (x, flagy2 - 10),\n",
        "                            (x + 25, flagy2 - 5),\n",
        "                        ],\n",
        "                    )\n",
        "                    gfxdraw.aapolygon(\n",
        "                        self.surf,\n",
        "                        [(x, flagy2), (x, flagy2 - 10), (x + 25, flagy2 - 5)],\n",
        "                        (204, 204, 0),\n",
        "                    )\n",
        "\n",
        "        self.surf = pygame.transform.flip(self.surf, False, True)\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            assert self.screen is not None\n",
        "            self.screen.blit(self.surf, (0, 0))\n",
        "            pygame.event.pump()\n",
        "            self.clock.tick(self.metadata[\"render_fps\"])\n",
        "            pygame.display.flip()\n",
        "        elif self.render_mode == \"rgb_array\":\n",
        "            return np.transpose(\n",
        "                np.array(pygame.surfarray.pixels3d(self.surf)), axes=(1, 0, 2)\n",
        "            )\n",
        "\n",
        "    def close(self):\n",
        "        if self.screen is not None:\n",
        "            import pygame\n",
        "\n",
        "            pygame.display.quit()\n",
        "            pygame.quit()\n",
        "            self.isopen = False\n",
        "\n",
        "\n",
        "def heuristic(env, s):\n",
        "    \"\"\"\n",
        "    The heuristic for\n",
        "    1. Testing\n",
        "    2. Demonstration rollout.\n",
        "\n",
        "    Args:\n",
        "        env: The environment\n",
        "        s (list): The state. Attributes:\n",
        "            s[0] is the horizontal coordinate\n",
        "            s[1] is the vertical coordinate\n",
        "            s[2] is the horizontal speed\n",
        "            s[3] is the vertical speed\n",
        "            s[4] is the angle\n",
        "            s[5] is the angular speed\n",
        "            s[6] 1 if first leg has contact, else 0\n",
        "            s[7] 1 if second leg has contact, else 0\n",
        "\n",
        "    Returns:\n",
        "         a: The heuristic to be fed into the step function defined above to determine the next step and reward.\n",
        "    \"\"\"\n",
        "\n",
        "    angle_targ = s[0] * 0.5 + s[2] * 1.0  # angle should point towards center\n",
        "    if angle_targ > 0.4:\n",
        "        angle_targ = 0.4  # more than 0.4 radians (22 degrees) is bad\n",
        "    if angle_targ < -0.4:\n",
        "        angle_targ = -0.4\n",
        "    hover_targ = 0.55 * np.abs(\n",
        "        s[0]\n",
        "    )  # target y should be proportional to horizontal offset\n",
        "\n",
        "    angle_todo = (angle_targ - s[4]) * 0.5 - (s[5]) * 1.0\n",
        "    hover_todo = (hover_targ - s[1]) * 0.5 - (s[3]) * 0.5\n",
        "\n",
        "    if s[6] or s[7]:  # legs have contact\n",
        "        angle_todo = 0\n",
        "        hover_todo = (\n",
        "            -(s[3]) * 0.5\n",
        "        )  # override to reduce fall speed, that's all we need after contact\n",
        "\n",
        "    if env.continuous:\n",
        "        a = np.array([hover_todo * 20 - 1, -angle_todo * 20])\n",
        "        a = np.clip(a, -1, +1)\n",
        "    else:\n",
        "        a = 0\n",
        "        if hover_todo > np.abs(angle_todo) and hover_todo > 0.05:\n",
        "            a = 2\n",
        "        elif angle_todo < -0.05:\n",
        "            a = 3\n",
        "        elif angle_todo > +0.05:\n",
        "            a = 1\n",
        "    return a\n",
        "\n",
        "\n",
        "def demo_heuristic_lander(env, seed=None, render=False):\n",
        "\n",
        "    total_reward = 0\n",
        "    steps = 0\n",
        "    s, info = env.reset(seed=seed)\n",
        "    while True:\n",
        "        a = heuristic(env, s)\n",
        "        s, r, terminated, truncated, info = step_api_compatibility(env.step(a), True)\n",
        "        total_reward += r\n",
        "\n",
        "        if render:\n",
        "            still_open = env.render()\n",
        "            if still_open is False:\n",
        "                break\n",
        "\n",
        "        if steps % 20 == 0 or terminated or truncated:\n",
        "            print(\"observations:\", \" \".join([f\"{x:+0.2f}\" for x in s]))\n",
        "            print(f\"step {steps} total_reward {total_reward:+0.2f}\")\n",
        "        steps += 1\n",
        "        if terminated or truncated:\n",
        "            break\n",
        "    if render:\n",
        "        env.close()\n",
        "    return total_reward\n",
        "\n",
        "\n",
        "class LunarLanderContinuous:\n",
        "    def __init__(self):\n",
        "        raise error.Error(\n",
        "            \"Error initializing LunarLanderContinuous Environment.\\n\"\n",
        "            \"Currently, we do not support initializing this mode of environment by calling the class directly.\\n\"\n",
        "            \"To use this environment, instead create it by specifying the continuous keyword in gym.make, i.e.\\n\"\n",
        "            'gym.make(\"LunarLander-v2\", continuous=True)'\n",
        "        )\n",
        "\n",
        "\n",
        "from gym.envs.registration import registry, register, make, spec\n",
        "\n",
        "register(\n",
        "    id='CustomLunarLander-v1',\n",
        "    entry_point='__main__:CustomLunarLander',\n",
        "    max_episode_steps=1000,\n",
        "    reward_threshold=300,\n",
        ")\n"
      ],
      "metadata": {
        "id": "HrZ5wQIQGt2h"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# new new new\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Sequential\n",
        "from collections import deque\n",
        "import numpy as np\n",
        "\n",
        "'''\n",
        "Param defs:\n",
        "env: gym env\n",
        "lr:  learning rate for gradient descent\n",
        "gamma: discount rate for future rewards in bellman function\n",
        "epsilon: exploration rate\n",
        "epsilon_decay: decay rate for exploration rate after each episode\n",
        "buffer_size: for experience replay...storing past information on states, actions, rewards, next_states, done\n",
        "'''\n",
        "\n",
        "class DoubleDQN(nn.Module):\n",
        "    def __init__(self, env, lr=1e-4, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, buffer_size=500000):\n",
        "        super(DoubleDQN, self).__init__()\n",
        "        # input output dimensions\n",
        "        self.state_space_dim = env.observation_space.shape[0]\n",
        "        self.action_space_dim = env.action_space.n\n",
        "\n",
        "        # our state-action function estimator\n",
        "\n",
        "        self.local_net = Sequential(\n",
        "            nn.Linear(self.state_space_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, self.action_space_dim)\n",
        "        )\n",
        "\n",
        "        self.target_net = Sequential(\n",
        "            nn.Linear(self.state_space_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, self.action_space_dim)\n",
        "        )\n",
        "        self.lr = lr\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.env = env\n",
        "        self.buffer = deque(maxlen=buffer_size)\n",
        "\n",
        "        # updates the weights of the model after computing gradients\n",
        "        self.optimizer = torch.optim.Adam(self.local_net.parameters(), lr=self.lr)\n",
        "\n",
        "        self.copy = 5 # Copy the local model weights into the target network every 2 steps\n",
        "        self.step = 0\n",
        "\n",
        "        self.loss_fn = nn.MSELoss()  # defining our loss function to be the MSE loss\n",
        "\n",
        "    def copy_model(self):\n",
        "        # Copy local net weights into target net\n",
        "        self.target_net.load_state_dict(self.local_net.state_dict())\n",
        "        self.target_net.eval()\n",
        "\n",
        "    def insert(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    # Get a mini-batch to train the model\n",
        "    def sample_buffer(self, num_samples):\n",
        "        states, actions, rewards, next_states, dones = [], [], [], [], []\n",
        "        idx = np.random.choice(len(self.buffer), num_samples)\n",
        "        for i in idx:\n",
        "            elem = self.buffer[i]\n",
        "            state, action, reward, next_state, done = elem\n",
        "            states.append(np.array(state, copy=False))\n",
        "            actions.append(np.array(action, copy=False))\n",
        "            rewards.append(reward)\n",
        "            next_states.append(np.array(next_state, copy=False))\n",
        "            dones.append(done)\n",
        "        states = torch.as_tensor(np.array(states))\n",
        "        actions = torch.as_tensor(np.array(actions, dtype=np.int64))\n",
        "        rewards = torch.as_tensor(np.array(rewards, dtype=np.float32))\n",
        "        next_states = torch.as_tensor(np.array(next_states))\n",
        "        dones = torch.as_tensor(np.array(dones, dtype=np.float32))\n",
        "        return states, actions, rewards, next_states, dones\n",
        "\n",
        "    def get_action(self, state):\n",
        "        self.step += 1\n",
        "        if np.random.uniform() < self.epsilon:\n",
        "            return self.env.action_space.sample()\n",
        "        else:\n",
        "            return np.argmax(self.local_net(state).data.numpy())\n",
        "\n",
        "    def train_batch(self, states, actions, rewards, next_states, dones):\n",
        "        if self.step % self.copy == 0:\n",
        "            self.copy_model()\n",
        "        # Bellman equation for updates\n",
        "        targets = rewards + self.gamma * self.target_net(next_states).max(-1).values * (1.0 - dones)\n",
        "        preds = self.local_net(states)\n",
        "        action_masks = F.one_hot(actions, self.action_space_dim)\n",
        "        preds = (preds * action_masks).sum(dim=-1)\n",
        "        loss = self.loss_fn(preds, targets.detach())\n",
        "        self.optimizer.zero_grad()  # zero out the gradients for weights of model\n",
        "        loss.backward()  # compute the gradient of loss with respect to model parameters\n",
        "        self.optimizer.step()\n",
        "        return loss"
      ],
      "metadata": {
        "id": "iE7MMcuMGu9p"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create environment\n",
        "env_name=\"CustomLunarLander-v2\"\n",
        "\n",
        "env=CustomLunarLander()\n",
        "env.render_mode=\"rgb_array\"\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JS4j5JcjHO3S"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "# from VanillaDqn import DQN\n",
        "# from DoubleDqn import DoubleDQN\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# env = gym.make('LunarLander-v2')\n",
        "# recorder = VideoRecorder(env, path='results/vanilladqn.mp4')\n",
        "episodes = 1000\n",
        "epsilon = 1.0\n",
        "gamma = 0.99\n",
        "buffer_size = 500000\n",
        "counter = 0\n",
        "update_t = 10\n",
        "batch_size = 128\n",
        "lr = 1e-3\n",
        "network = DoubleDQN(env, lr=lr, gamma=gamma, epsilon=epsilon, buffer_size=buffer_size)\n",
        "reward_list_ep = []\n",
        "reward_last_100_eps = []\n",
        "\n",
        "for episode in range(episodes + 1):\n",
        "    state= env.reset()[0].astype(np.float32)\n",
        "    # print(state)\n",
        "    reward_ep, done = 0, False\n",
        "\n",
        "    while not done:\n",
        "        # env.render()  # Comment this if you do not want rendering\n",
        "        # env.unwrapped.render() # Comment this if you do not want rendering\n",
        "        # recorder.capture_frame()\n",
        "        state_tensor = torch.from_numpy(np.array(state))\n",
        "        action = network.get_action(state_tensor)\n",
        "        # print(action)\n",
        "        # print(env.step(action)/)\n",
        "        next_state, reward, done, info,_ = env.step(action)\n",
        "        next_state = next_state.astype(np.float32)\n",
        "        reward_ep += reward\n",
        "        network.insert(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "\n",
        "        counter += 1\n",
        "        counter %= update_t\n",
        "\n",
        "        if len(network.buffer) > batch_size:  # update weights every 5 steps\n",
        "            states, actions, rewards, next_states, dones = network.sample_buffer(batch_size)\n",
        "            loss = network.train_batch(states, actions, rewards, next_states, dones)\n",
        "\n",
        "    if episode < 900:\n",
        "        network.epsilon *= network.epsilon_decay\n",
        "    reward_list_ep.append(reward_ep)\n",
        "\n",
        "    if len(reward_last_100_eps) == 100:\n",
        "        reward_last_100_eps = reward_last_100_eps[1:]\n",
        "    reward_last_100_eps.append(reward_ep)\n",
        "\n",
        "    if episode % 50 == 0 and episode > 1:\n",
        "        print(f'Episode {episode}/{episodes}. Epsilon: {network.epsilon:.3f}.'\n",
        "              f' Reward in the last 100 episodes: {np.mean(reward_last_100_eps):.2f}')\n",
        "    last_rewards_mean = np.mean(reward_last_100_eps)\n",
        "    # if last_rewards_mean > 200:\n",
        "    #     break\n",
        "env.close()\n",
        "\n",
        "# PATH1 = 'saved/lunarlanderDouble.pt'\n",
        "# torch.save(network.state_dict(), PATH1)\n",
        "fig = plt.figure(figsize=(20, 10))\n",
        "#plt.scatter([i for i in range(len(reward_list_ep))], reward_list_ep)\n",
        "plt.xlabel(\"Episodes\")\n",
        "plt.ylabel(\"Rewards\")\n",
        "plt.plot([i for i in range(len(reward_list_ep))], reward_list_ep)\n",
        "plt.xlabel(\"Episodes\")\n",
        "plt.ylabel(\"Rewards\")\n",
        "plt.savefig('DoubleDQN')\n",
        "\n",
        "# fig = plt.figure(figsize=(20, 10))\n",
        "# plt.scatter([i for i in range(len(reward_list_ep))], reward_list_ep)\n",
        "# plt.xlabel(\"Episodes\")\n",
        "# plt.ylabel(\"Rewards\")\n",
        "# plt.savefig('results/vanillaDQNscatter.png')\n",
        "# plt.plot([i for i in range(len(reward_list_ep))], reward_list_ep)\n",
        "# plt.xlabel(\"Episodes\")\n",
        "# plt.ylabel(\"Rewards\")\n",
        "# plt.savefig('results/vanillaDQNplot.png')\n",
        "\n",
        "# Specify a path\n",
        "PATH = \"saved/lunarlanderDouble_modified_reward.pt\"\n",
        "\n",
        "# Save\n",
        "torch.save(network, PATH)\n",
        "\n",
        "fig = plt.figure(figsize=(20, 10))\n",
        "plt.plot([i for i in range(len(reward_list_ep))], reward_list_ep)\n",
        "plt.xlabel(\"Episodes\")\n",
        "plt.ylabel(\"Rewards\")\n",
        "plt.savefig('DoubleDQNscatte_mod_reward.png')\n",
        "plt.plot([i for i in range(len(reward_list_ep))], reward_list_ep, )\n",
        "plt.xlabel(\"Episodes\")\n",
        "plt.ylabel(\"Rewards\")\n",
        "plt.savefig('DoubleDQNscatter_mod_reward')\n",
        "\n",
        "\n",
        "files.download('/content/saved/lunarlanderDouble_modified_reward.pt')"
      ],
      "metadata": {
        "id": "mZncd4bdG-WC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the trained actor\n",
        "import gym\n",
        "# from VanillaDqn import DQN\n",
        "# from DoubleDqn import DoubleDQN\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# env = gym.make('LunarLander-v2')\n",
        "# recorder = VideoRecorder(env, path='results/vanilladqn.mp4')\n",
        "\n",
        "# Create environment\n",
        "env_name=\"CustomLunarLander-v2\"\n",
        "\n",
        "env=CustomLunarLander()\n",
        "env.render_mode=\"rgb_array\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "episodes = 1000\n",
        "epsilon = 1.0\n",
        "gamma = 0.99\n",
        "buffer_size = 500000\n",
        "counter = 0\n",
        "update_t = 10\n",
        "batch_size = 128\n",
        "lr = 1e-3\n",
        "network = DoubleDQN(env, lr=lr, gamma=gamma, epsilon=epsilon, buffer_size=buffer_size)\n",
        "\n",
        "network.load_state_dict(torch.load(\"/content/lunarlanderDouble_modified_reward.pt\"))\n",
        "network.eval()\n",
        "\n",
        "# loaded_actor = torch.load(\"/content/lunarlanderDouble_modified_reward.pt\")\n"
      ],
      "metadata": {
        "id": "CMiu-UFlQSwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "\n",
        "\n",
        "vid = VideoRecorder(env, path=f\"video/{env_name}_modified_learned.mp4\")\n",
        "observation = env.reset()[0]\n",
        "\n",
        "\n",
        "total_reward = 0\n",
        "done = False\n",
        "while not done:\n",
        "  frame = env.render()\n",
        "  # print(frame)\n",
        "  vid.capture_frame()\n",
        "  state_tensor = torch.from_numpy(observation)\n",
        "  action = network.get_action(state_tensor)\n",
        "  observation, reward, done, info,_ = env.step(action)\n",
        "  total_reward += reward\n",
        "vid.close()\n",
        "env.close()\n",
        "print(f\"\\nTotal reward: {total_reward}\")\n",
        "\n",
        "# show video\n",
        "html = render_mp4(f\"video/{env_name}_modified_learned.mp4\")\n",
        "HTML(html)"
      ],
      "metadata": {
        "id": "td4yqJ4tL0MX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}